# etl_page.py
import os
import streamlit as st
import PyPDF2
from groq import Groq
import requests
import faiss
import numpy as np
import pandas as pd
import torch

# Streamlitãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤ºæƒ…å ±
st.set_page_config(page_title="ETL Page", page_icon="ğŸ”„")

st.title("ğŸ”„ETL Processing")
st.write("This page is dedicated to ETL (Extract, Transform, Load) operations.")

# Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
api_key = os.getenv("GROQ_API_KEY")
if not api_key:
    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()



# PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")
extract_items = []
num_pages = 0
faiss_index_db = None

# ETLå‡¦ç†ã®é–¢æ•°
def extract_process(pdf_file):
    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    with open(pdf_file.name, "wb") as f:
        f.write(pdf_file.getbuffer())

    file_title = pdf_file.name

    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    with open(pdf_file.name, "rb") as f:
        pdf_reader = PyPDF2.PdfReader(f)
        num_pages = len(pdf_reader.pages)
        st.write(f"Number of pages: {num_pages}")

        # å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        for page_num in range(num_pages):
            page = pdf_reader.pages[page_num]
            text = page.extract_text()
            # ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§ä½œæˆ
            page_item = {
                "title": file_title,
                "pageNumber": page_num + 1,
                "text": text
            }
            # extract_itemsãƒªã‚¹ãƒˆã«ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’è¿½åŠ 
            extract_items.append(page_item)
            
            st.write(f"Page {page_num+1}: {text}")

    st.success("ETL process completed. Continue to Transform Process.")
    return True

def transform_process():
    groq_client = Groq(api_key=api_key)
    # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    api_url = "http://localhost:5000/encoding"

    # st.write(extract_items)
    items_embeddings = torch.tensor([])
    for item in extract_items:
        st.write(f"{item}")
        # Groq APIã‚’ä½¿ç”¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±ã«å¤‰æ›
        # vector = groq_client.encode(item['text'])
        # st.write(f"Vector representation:{vector}")
        # response = groq_client.embeddings.create(
        #     # model="nomic-embed-text-v1.5",
        #     model="ada-30b-4096",
        #     input=item['text']
        # )
        # st.write("Vector representation:")
        # st.write(response['data'])
        sentences = item['text']
        response = requests.post(api_url, json={"sentences": sentences})
        if response.status_code == 200:
            embeddings = response.json().get("embeddings", [])
            st.write("Vector representation for page", item['pageNumber'])
            st.write(embeddings)
            # NumPyé…åˆ—ã«å¤‰æ›ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
            embeddings_np = np.array(embeddings)
            embeddings_tensor = torch.tensor(embeddings_np).float()
            # Tensorã«è¿½åŠ 
            items_embeddings = torch.cat((items_embeddings, embeddings_tensor.unsqueeze(0)), dim=0)
        else:
            st.error("Failed to get embeddings from API")
            return False

    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    if items_embeddings.size(0) > 0:
        # dimension = items_embeddings[0].shape[1]
        # dimension = len(items_embeddings[0])
        dimension = items_embeddings.size(1)
        faiss_index_db = faiss.IndexFlatL2(dimension)

        # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›ã—ã¦FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        # items_embeddings_np = np.vstack(items_embeddings)
        # faiss_index_db.add(items_embeddings_np)
        faiss_index_db.add(items_embeddings.detach().cpu().numpy())

        st.success("FAISS index created successfully!")
        # st.write(faiss_index_db)

    return True

def save_data_process():
    st.error("Save function has not been implemented!")
    return False


# ETLå‡¦ç†ã®å®Ÿè¡Œ
if uploaded_file is None:
    st.warning("Please upload a PDF file first.")
else:
    if st.button("Run ETL"):
        with st.status("ETL processing ...", expanded=True) as status:
            extract_items = []
            num_pages = 0
            if extract_process(uploaded_file) == False:
                st.warning("Extract Process is failed.")
                st.stop()

            if transform_process() == False:
                st.warning("Transform Process is failed.")
                st.stop()

            if save_data_process() == False:
                st.warning("Save Process is failed.")
                st.stop()

            st.success("ETL process finished!")
