# etl_page.py
import os
from datetime import datetime
import streamlit as st
import PyPDF2
from groq import Groq
import requests
import faiss
import numpy as np
import pandas as pd
import torch
import tempfile

# Streamlitãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤ºæƒ…å ±
st.set_page_config(page_title="ETL Page", page_icon="ğŸ”„")

# ETLãƒšãƒ¼ã‚¸é–¢é€£ã®æƒ…å ±ã®åˆæœŸåŒ–
if "collection_name" not in st.session_state:
    st.session_state.collection_name = ""
    st.session_state.etl_success = False
    st.session_state.extract_items = []

# APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
api_base_url = "http://localhost:5000"

def check_exist_embedding_server():
    echo_path = api_base_url + "/echo"
    try:
        response = requests.get(echo_path)
        if response.status_code == 200:
            print("Exist Embedding Server")
            # st.success("Exist Embedding Server")
            return True
        else:
            print("No Embedding Server")
            st.error("No Embedding Server")
            return False
    except Exception as e:
        print(f"Failed to access server: {e}")  # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆæ–‡
        return False

# ETLå‡¦ç†ã®é–¢æ•°
def extract_process(pdf_file):
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    temporal_dir = "temporal_storage"
    os.makedirs(temporal_dir, exist_ok=True)
    with tempfile.TemporaryDirectory(dir=temporal_dir) as temp_dir:
        temp_pdf_path = os.path.join(temp_dir, pdf_file.name)

        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        with open(temp_pdf_path, "wb") as f:
            f.write(pdf_file.getbuffer())

        file_title = pdf_file.name

        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
        with open(temp_pdf_path, "rb") as f:
            pdf_reader = PyPDF2.PdfReader(f)
            num_pages = len(pdf_reader.pages)
            st.write(f"Number of pages: {num_pages}")

            # å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            for page_num in range(num_pages):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                # ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§ä½œæˆ
                page_item = {
                    "title": file_title,
                    "pageNumber": page_num + 1,
                    "text": text,
                }
                # extract_itemsãƒªã‚¹ãƒˆã«ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’è¿½åŠ 
                extract_items.append(page_item)
                st.session_state.extract_items.append(page_item)

                st.write(f"Page {page_num+1}: {text}")

        # st.session_state.extract_items = extract_items
        st.success("Extract process completed. Continue to Transform Process.")
        return True


def transform_process(collection_name):
    # embedding_url = api_base_url + "/encoding"

    # ãƒ™ã‚¯ãƒˆãƒ«å€¤ã®å–å¾—
    items_embeddings = torch.tensor([])
    texts = []
    for item in extract_items:
        # st.write(f"{item}")
        sentence = item["text"]
        # st.write(sentence)
        texts.append(sentence)

    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    create_index_url = api_base_url + "/collections/" + st.session_state.collection_name
    print(f"put URL is {create_index_url}")
    if len(texts) > 0:
        request_body = {"texts": texts}
        response = requests.put(create_index_url, json=request_body)
        print(f"response_state is {response.status_code}")
        if response.status_code == 201:
            print("FAISS index created successfully!")
            st.success("Transfer process completed.")
        else:
            print("Failed to create FAISS index")
            st.error("Failed to create FAISS index")
            return False

    return True


# def save_data_process():
#     st.error("Save function has not been implemented!")
#     return False


# Streamlit ã®ãƒšãƒ¼ã‚¸ç”Ÿæˆ
st.title("ğŸ”„ETL Processing")
st.write("This page is dedicated to ETL (Extract, Transform, Load) operations.")


# PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
has_embedding_server = check_exist_embedding_server()
if not has_embedding_server:
    st.error("This page needs embedding-server")
else:
    uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")
    extract_items = []
    num_pages = 0
    faiss_index_db = None

    st.session_state.etl_success = False


    # ETLå‡¦ç†ã®å®Ÿè¡Œ
    if uploaded_file is None:
        st.warning("Please upload a PDF file first.")
    else:
        if st.button("Run ETL"):
            st.session_state.etl_success = False
            with st.status("ETL processing ...", expanded=True) as status:
                extract_items = []
                st.session_state.extract_items = []
                num_pages = 0
                if extract_process(uploaded_file) == False:
                    st.warning("Extract Process is failed.")
                    st.stop()

                # define collection_name
                now = datetime.now()
                collection_name = now.strftime("%y%m%d-%H%M%S")  # %yã§2æ¡ã®å¹´ã€%Hã§24hè¡¨è¨˜
                st.session_state.collection_name = collection_name

                if transform_process(collection_name) == False:
                    st.warning("Transform Process is failed.")
                    st.stop()

                # if save_data_process() == False:
                #     st.warning("Save Process is failed.")
                #     st.stop()
                st.session_state.etl_success = True
                st.success(f"ETL process success! - collection : {collection_name} -")

